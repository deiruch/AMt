{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "import panel as pn \n",
    "import re\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_function import *\n",
    "from plot_function import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set audio snippets and sample compression\n",
    "offset_sec = 0.01\n",
    "duration_sec = 0.05\n",
    "num_data_points = 50\n",
    "\n",
    "# Create empty Dataframe\n",
    "df_cols = ['dataset', 'audio_file_name', 'pitch', 'onset_sec', 'amplitude', 'frequency']\n",
    "path_main = \"IDMT-SMT-GUITAR_V2_MOD/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframe with reference tones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_ref = pd.DataFrame(columns=df_cols)\n",
    "path_main_ref = path_main + \"dataset1/Fender Strat Clean Neck SC/\"\n",
    "df_ref = read_xml_to_df(path_main_ref, df_cols, offset_sec, duration_sec, num_data_points)\n",
    "df_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch = 18\n",
    "plot_freq_domain(df_ref.frequency[pitch], df_ref.amplitude[pitch], df_ref.pitch[pitch])\n",
    "print('Datapoints: ', len(df_ref.amplitude[pitch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with unique pitches (only one is used if the pitches are the same)\n",
    "df_ref_unique = pd.DataFrame(columns=df_cols)\n",
    "\n",
    "for index, row in df_ref.iterrows():\n",
    "    if not df_ref_unique.pitch.str.contains(row.pitch[0], regex=False).any():\n",
    "        df_ref_unique = df_ref_unique.append(row, ignore_index=True)\n",
    "df_ref_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with averaged pitches\n",
    "df_ref_merge = pd.DataFrame(columns=df_cols)\n",
    "for index, row in df_ref.iterrows():\n",
    "    if not df_ref_merge.pitch.str.contains(row.pitch[0], regex=False).any():\n",
    "        df_ref_merge = df_ref_merge.append(row, ignore_index=True)\n",
    "    else:\n",
    "        ind = df_ref_merge.pitch.str.contains(row.pitch[0], regex=False).tolist().index(True)\n",
    "        amplitude_a = df_ref_merge.amplitude[ind]\n",
    "        amplitude_b = row.amplitude\n",
    "        amplitude_sum = np.asarray([sum(x) for x in zip(amplitude_a, amplitude_b)])\n",
    "        df_ref_merge.at[ind, 'amplitude'] = amplitude_sum\n",
    "\n",
    "# normalise\n",
    "for index, row in df_ref_merge.iterrows():\n",
    "    df_ref_merge.at[index, 'amplitude'] = norm_vec(row.amplitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign reference dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ref_mod = df_ref_unique\n",
    "df_ref_mod = df_ref_merge\n",
    "df_ref_mod.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframe with testdatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_test = pd.DataFrame(columns=df_cols)\n",
    "\n",
    "path_testdata = [\n",
    "    path_main + 'dataset1/Fender Strat Clean Neck SC/',\n",
    "    path_main + 'dataset1/Fender Strat Clean Neck SC Chords/',\n",
    "    path_main + 'dataset1/Ibanez Power Strat Clean Bridge HU/',\n",
    "    path_main + 'dataset1/Ibanez Power Strat Clean Bridge HU Chords/',\n",
    "    path_main + 'dataset1/Ibanez Power Strat Clean Bridge+Neck SC/',\n",
    "    path_main + 'dataset1/Ibanez Power Strat Clean Neck HU/',\n",
    "    path_main + 'dataset2/',\n",
    "    path_main + 'datasetRecording/']\n",
    "\n",
    "#for path in path_testdata[:7]:\n",
    "for path in path_testdata:\n",
    "    df_act = read_xml_to_df(path, df_cols, offset_sec, duration_sec, num_data_points)\n",
    "    df_test = df_test.append(df_act, ignore_index=True)\n",
    "\n",
    "df_test['mono_poly'] = df_test.pitch.apply(lambda x: len(x))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch = 6\n",
    "plot_freq_domain(df_test.frequency[pitch], df_test.amplitude[pitch], df_test.pitch[pitch])\n",
    "print('Datapoints: ', len(df_ref.amplitude[pitch]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of Pitches Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pitches_temp = df_test.pitch.tolist()\n",
    "\n",
    "# flatten List\n",
    "all_pitches = []\n",
    "for sublist in all_pitches_temp:\n",
    "    for item in sublist:\n",
    "        all_pitches.append(item)\n",
    "\n",
    "plot_hist(all_pitches, min(df_ref_mod.pitch), max(df_ref_mod.pitch), 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc all target vectors\n",
    "target_pitches = df_test.pitch.array\n",
    "target_vec = []\n",
    "\n",
    "for pitch in target_pitches:\n",
    "    target_vec.append(calc_target_vec(pitch))\n",
    "\n",
    "df_test['target_vec'] = target_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gurobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "m = Model('AMt')\n",
    "\n",
    "lambs = []\n",
    "#bins = []\n",
    "signal = []\n",
    "for lamb in range(len(df_ref_mod)):\n",
    "    lambs.append(m.addVar(lb = 0, ub = 1, vtype = GRB.CONTINUOUS, name = 'lamb_' + str(lamb)))\n",
    "    #bins.append(m.addVar(vtype = GRB.BINARY, name = 'bin_' + str(lamb)))\n",
    "\n",
    "amplitudes = df_test.amplitude.tolist()\n",
    "len_amplitude = len(amplitudes[0])\n",
    "\n",
    "for datapoint in range(len_amplitude):\n",
    "    signal.append(m.addVar(lb = 0, ub = 1, vtype = GRB.CONTINUOUS, name = 'x_' + str(datapoint)))\n",
    "    \n",
    "m.update()\n",
    "m.printStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vec = []\n",
    "grb_time = 0\n",
    "alpha = 1.3\n",
    "\n",
    "len_lambs = len(lambs)\n",
    "ref_tones = df_ref_mod.amplitude.tolist()\n",
    "deviation = m.addVar(lb = -37, ub = 1, vtype = GRB.CONTINUOUS) # +1 Continuous\n",
    "\n",
    "##Constraints\n",
    "#for i in range(len_lambs):\n",
    "#    m.addConstr(lambs[i] <= bins[i]) # +37 Constrs, + 2*37 NZs\n",
    "#    \n",
    "#m.addConstr(quicksum(bins) <= 6) # +1 Constrs, +37NZs\n",
    "\n",
    "m.update()\n",
    "m.printStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for amplitude in amplitudes:\n",
    "    cost_function = 0\n",
    "    penalty = 0\n",
    "    \n",
    "    for i in range(len_lambs):\n",
    "        #penalty += lambs[i] * lambs[i]\n",
    "        penalty += lambs[i]\n",
    "    \n",
    "    for f in range(len_amplitude):\n",
    "        signal[f].lb = signal[f].ub = amplitude[f]\n",
    "    \n",
    "    for f in range(len_amplitude):\n",
    "        approx_sgn = 0\n",
    "        \n",
    "        for r in range(len_lambs):\n",
    "            approx_sgn += ref_tones[r][f] * lambs[r]\n",
    "\n",
    "        deviation = signal[f] - approx_sgn\n",
    "        cost_function += deviation * deviation\n",
    "    \n",
    "    #Params\n",
    "    m.params.outputflag = 0 #infotext\n",
    "    #m.params.Presolve = 0\n",
    "    \n",
    "    m.setObjective(cost_function + alpha * penalty, GRB.MINIMIZE)\n",
    "    m.optimize()\n",
    "    \n",
    "    grb_time += m.Runtime\n",
    "    approx_v = []\n",
    "\n",
    "    for v in m.getVars():\n",
    "        #print('%s: %g' % (v.varName, v.x))\n",
    "        if re.match(r'lamb', v.varName):\n",
    "            approx_v.append(v.x)\n",
    "\n",
    "    pred_vec.append(approx_v)\n",
    "    \n",
    "df_test['pred_vec'] = pred_vec\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pred_vec\n",
    "%time\n",
    "norm_pred_vec = []\n",
    "\n",
    "for j in range(len(df_test.pred_vec)):\n",
    "    thresh_vec = []\n",
    "    vec = norm_vec(df_test.pred_vec[j])        \n",
    "    norm_pred_vec.append(vec)\n",
    "\n",
    "df_test['norm_pred_vec'] = norm_pred_vec\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Pitch Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_score = []\n",
    "\n",
    "for vec in range(len(pred_vec)):\n",
    "    pitch_score.append(metric(target_vec[vec], np.asarray(norm_pred_vec[vec])))\n",
    "    \n",
    "df_test['pitch_score'] = pitch_score\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "threshold = 0\n",
    "\n",
    "combi_target_vec = []\n",
    "for i in df_test.target_vec:\n",
    "    for elem in i:\n",
    "        combi_target_vec.append(elem)\n",
    "\n",
    "f1_dict = {}\n",
    "while threshold <= 1:\n",
    "    thresholded_vec = []\n",
    "\n",
    "    for j in range(len(df_test.norm_pred_vec)):\n",
    "        thresh_vec = []\n",
    "        vec = df_test.norm_pred_vec[j]\n",
    "\n",
    "        for i in range(len(vec)):\n",
    "            if vec[i] >= threshold:\n",
    "                thresh_vec.append(1)\n",
    "            else:\n",
    "                thresh_vec.append(0)\n",
    "\n",
    "        thresholded_vec.append(thresh_vec)\n",
    "\n",
    "        \n",
    "    combi_pred_vec = []\n",
    "    for i in thresholded_vec:\n",
    "\n",
    "        for elem in i:\n",
    "            combi_pred_vec.append(elem)\n",
    "    \n",
    "    f1 = round(f1_score(combi_target_vec, combi_pred_vec), 2)\n",
    "    #print('F1 score: {}, Threshold: {}'.format(f1, threshold))\n",
    "    f1_dict.update( {threshold : f1} )\n",
    "    threshold = round(threshold + 0.01, 2)\n",
    "\n",
    "threshold = max(f1_dict.items(), key=operator.itemgetter(1))[0]\n",
    "print('Best threshold: ', threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc vector with best threshold\n",
    "thresholded_vec = []\n",
    "pitch_pred = []\n",
    "\n",
    "for j in range(len(df_test.norm_pred_vec)):\n",
    "    thresh_vec = []\n",
    "    vec = df_test.norm_pred_vec[j]\n",
    "    \n",
    "    for i in range(len(vec)):\n",
    "        if vec[i] >= threshold:\n",
    "            thresh_vec.append(1)\n",
    "        else:\n",
    "            thresh_vec.append(0)\n",
    "            \n",
    "    thresholded_vec.append(thresh_vec)\n",
    "    pitch_pred.append(vec_to_pitch(thresh_vec))\n",
    "    \n",
    "df_test['thresholded_vec'] = thresholded_vec\n",
    "df_test['pitch_pred'] = pitch_pred\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pitch = 1\n",
    "plot_bar(df_test, test_pitch, 10, 4, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test.to_pickle('all_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = pd.read_pickle('all_datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combi_target_vec = []\n",
    "for i in df_test.target_vec:\n",
    "    for elem in i:\n",
    "        combi_target_vec.append(elem)\n",
    "\n",
    "combi_pred_vec = []\n",
    "for i in df_test.thresholded_vec:\n",
    "\n",
    "    for elem in i:\n",
    "        combi_pred_vec.append(elem)\n",
    "#print(round(f1_score(combi_target_vec, combi_pred_vec), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_score = df_test.loc[df_test.mono_poly == 1].pitch_score\n",
    "poly_score = df_test.loc[df_test.mono_poly > 1].pitch_score\n",
    "plot_box(mono_score, poly_score, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_pitches = []\n",
    "mono_pitches_pred = []\n",
    "undetected = 95\n",
    "df_mono = df_test.loc[df_test.mono_poly == 1]\n",
    "\n",
    "for index, row in df_mono.iterrows():\n",
    "    len_pitch_pred = len(row.pitch_pred) \n",
    "    #print('Index: {}, Pitch: {}, LÃ¤nge: {}'.format(index, row.pitch, len_pitch_pred))\n",
    "    \n",
    "    if len_pitch_pred == 0:\n",
    "        mono_pitches.append(row.pitch)\n",
    "        mono_pitches_pred.append(undetected)\n",
    "    else:\n",
    "        for item in row.pitch_pred:\n",
    "            mono_pitches.append(row.pitch)\n",
    "            mono_pitches_pred.append(item)\n",
    "            \n",
    "plot_scatter(mono_pitches_pred, mono_pitches, 6, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_datasets = df_test.dataset.unique()\n",
    "text_dataset = ''\n",
    "    \n",
    "for i, text in enumerate(used_datasets):\n",
    "    text_dataset += text\n",
    "    if i < len(used_datasets)-1:\n",
    "        text_dataset += '<br>'\n",
    "    else:\n",
    "        text_dataset += '</p>'\n",
    "\n",
    "tbl_dataset = (\n",
    "'<table>'\n",
    "    '<tr>'\n",
    "        '<th>test dataset:</th>'\n",
    "    '</tr>'\n",
    "    '<tr>'\n",
    "        '<td>' + text_dataset + '</td>'\n",
    "    '</tr>'\n",
    "'</table>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(combi_target_vec, combi_pred_vec, output_dict=True)\n",
    "zero = '0'\n",
    "one = '1'\n",
    "macro = 'macro avg'\n",
    "weight = 'weighted avg'\n",
    " \n",
    "class_report = (\n",
    "'<table>'\n",
    "  '<tr>'\n",
    "    '<th></th>'\n",
    "    '<th>precision</th>'\n",
    "    '<th>recall</th>'\n",
    "    '<th>f1-score</th>'\n",
    "  '</tr>'\n",
    "  '<tr>'\n",
    "    '<td>' + zero +'</td>'\n",
    "    '<td>' + str(round(report[zero]['precision'], 2)) + '</td>'\n",
    "    '<td>' + str(round(report[zero]['recall'], 2)) + '</td>'\n",
    "    '<td>' + str(round(report[zero]['f1-score'], 2)) + '</td>'\n",
    "  '</tr>'\n",
    "  '<tr>'\n",
    "    '<td>' + one +'</td>'\n",
    "    '<td>' + str(round(report[one]['precision'], 2)) + '</td>'\n",
    "    '<td>' + str(round(report[one]['recall'], 2)) + '</td>'\n",
    "    '<td>' + str(round(report[one]['f1-score'], 2)) + '</td>'\n",
    "  '</tr>'\n",
    "  '<tr>'\n",
    "    '<td>accuracy</td>'\n",
    "    '<td>''</td>'\n",
    "    '<td>''</td>'\n",
    "    '<td>' + str(round(report['accuracy'], 2)) + '</td>'\n",
    "  '</tr>'\n",
    "  '<tr>'\n",
    "    '<td>' + macro +'</td>'\n",
    "    '<td>' + str(round(report[macro]['precision'], 2)) + '</td>'\n",
    "    '<td>' + str(round(report[macro]['recall'], 2)) + '</td>'\n",
    "    '<td>' + str(round(report[macro]['f1-score'], 2)) + '</td>'\n",
    "  '</tr>'\n",
    "  '<tr>'\n",
    "    '<td>' + weight +'</td>'\n",
    "    '<td>' + str(round(report[weight]['precision'], 2)) + '</td>'\n",
    "    '<td>' + str(round(report[weight]['recall'], 2)) + '</td>'\n",
    "    '<td>' + str(round(report[weight]['f1-score'], 2)) + '</td>'\n",
    "  '</tr>'\n",
    "'</table> '\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_count = len(df_test.loc[df_test.mono_poly==1])\n",
    "poly_count = len(df_test.loc[df_test.mono_poly>1])\n",
    "\n",
    "text_info = (\n",
    "'''\n",
    "<style>\n",
    "table, th, td {\n",
    "    font-size: 11px;\n",
    "    text-align: left;\n",
    "}\n",
    "    \n",
    "th, td {\n",
    "    padding: 0px;\n",
    "}\n",
    "</style>\n",
    "'''\n",
    "\n",
    "'<table>'\n",
    "  '<tr>'\n",
    "    '<th>number of monophonics:</th>'\n",
    "    '<td>' + str(mono_count) + '</td>'\n",
    "  '</tr>'\n",
    "  '<tr>'\n",
    "    '<th>number of polyphonics:</th>'\n",
    "    '<td>' + str(poly_count) + '</td>'\n",
    "  '</tr>'\n",
    "  '<tr>'\n",
    "    '<th>audio offset [s]:</th>'\n",
    "    '<td>' + str(offset_sec) + '</td>'\n",
    "  '</tr>'\n",
    "  '<tr>'\n",
    "    '<th>data points of audiosnippets:</th>'\n",
    "    '<td>' + str(len(df_ref_mod.amplitude[0])) + '</td>'\n",
    "  '</tr>'\n",
    "  '<tr>'\n",
    "    '<th>length of audiosnippets [s]:</th>'\n",
    "    '<td>' + str(duration_sec) + '</td>'\n",
    "  '</tr>'\n",
    "  '<tr>'\n",
    "    '<th>Gurobi regularisation factor:</th>'\n",
    "    '<td>' + str(alpha) + '</td>'\n",
    "  '</tr>'\n",
    "  '<tr>'\n",
    "    '<th>Gurobi calculation time [s]:</th>'\n",
    "    '<td>' + str(round(grb_time, 2)) + '</td>'\n",
    "  '</tr>'\n",
    "  '<tr>'\n",
    "    '<th>Threshold:</th>'\n",
    "    '<td>' + str(threshold) + '</td>'\n",
    "  '</tr>'\n",
    "'</table> '\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dashboard = '<div style=\"font-size:35px\">Dashboard</div>'\n",
    "title_info = '<div style=\"font-size:24px\">General information</div>'\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_1 = 1\n",
    "pitch_2 = 103\n",
    "\n",
    "dashboard = pn.Column(pn.Row(title_dashboard),\n",
    "                      pn.Row(plot_hist(all_pitches, min(df_ref_mod.pitch), max(df_ref_mod.pitch),10, 4),\n",
    "                             pn.Column(title_info,\n",
    "                                       pn.Row(tbl_dataset,\n",
    "                                              text_info,\n",
    "                                             ),\n",
    "                                      ),\n",
    "                            ),\n",
    "                      pn.Row(pn.Column(plot_bar(df_test, pitch_1, 10, 3, threshold), \n",
    "                                       plot_bar(df_test, pitch_2, 10, 3, threshold),\n",
    "                                      ),\n",
    "                             pn.Column(plot_scatter(mono_pitches_pred, mono_pitches, 6, 6)),\n",
    "                             pn.Column(plot_box(mono_score, poly_score, 4, 4), class_report),\n",
    "                            ),\n",
    "                     )\n",
    " \n",
    "dashboard.show()\n",
    "#dashboard.servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
